# ä»£ç ä¼˜å…ˆå·¥ä½œæµæ¡†æ¶æ¨è

Activepieces æ˜¯ä¸€ä¸ªä½ä»£ç å·¥ä½œæµè‡ªåŠ¨åŒ–å¹³å°ï¼ˆç±»ä¼¼ Zapierã€n8nï¼‰ï¼Œå¦‚æœä½ éœ€è¦**ä»£ç ä¼˜å…ˆ**çš„ç±»ä¼¼æ¡†æ¶ï¼Œä»¥ä¸‹æ˜¯æ¨èé€‰é¡¹ï¼š

---

## ğŸ¯ ä¸»æµä»£ç ä¼˜å…ˆå·¥ä½œæµæ¡†æ¶

### 1. **Prefect** â­â­â­â­â­ï¼ˆæœ€æ¨èï¼‰

**ç‰¹ç‚¹ï¼š**
- âœ… **çº¯ Python ä»£ç **ï¼šå®Œå…¨ä»£ç ä¼˜å…ˆï¼Œæ—  UI ä¾èµ–
- âœ… **ç°ä»£åŒ–è®¾è®¡**ï¼šAPI ç®€æ´ä¼˜é›…ï¼Œæ˜“äºä½¿ç”¨
- âœ… **å¼ºå¤§çš„è°ƒåº¦**ï¼šæ”¯æŒå¤æ‚è°ƒåº¦è§„åˆ™å’Œæ¡ä»¶è§¦å‘
- âœ… **çŠ¶æ€ç®¡ç†**ï¼šå†…ç½®çŠ¶æ€è·Ÿè¸ªå’Œé‡è¯•æœºåˆ¶
- âœ… **äº‘åŸç”Ÿ**ï¼šæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²
- âœ… **ä¸°å¯Œçš„é›†æˆ**ï¼šæ”¯æŒå„ç§æ•°æ®æºå’Œ API

**é€‚ç”¨åœºæ™¯ï¼š**
- æ•°æ®ç®¡é“å’Œå·¥ä½œæµç¼–æ’
- ETL ä»»åŠ¡
- API é›†æˆå’Œè‡ªåŠ¨åŒ–
- å®šæ—¶ä»»åŠ¡å’Œè°ƒåº¦

**ä»£ç ç¤ºä¾‹ï¼š**
```python
from prefect import flow, task
from prefect.tasks import task_input_hash
from datetime import timedelta

@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
def fetch_data():
    # è·å–æ•°æ®
    return data

@task
def process_data(data):
    # å¤„ç†æ•°æ®
    return processed_data

@task
def send_notification(result):
    # å‘é€é€šçŸ¥
    pass

@flow
def my_workflow():
    data = fetch_data()
    processed = process_data(data)
    send_notification(processed)

if __name__ == "__main__":
    my_workflow()
```

**æ¨èæŒ‡æ•°ï¼šâ­â­â­â­â­**

---

### 2. **Apache Airflow** â­â­â­â­

**ç‰¹ç‚¹ï¼š**
- âœ… **DAG å®šä¹‰**ï¼šä½¿ç”¨ Python ä»£ç å®šä¹‰æœ‰å‘æ— ç¯å›¾
- âœ… **æˆç†Ÿç¨³å®š**ï¼šå¹¿æ³›ä½¿ç”¨ï¼Œç¤¾åŒºåºå¤§
- âœ… **ä¸°å¯Œçš„æ“ä½œç¬¦**ï¼šæ”¯æŒå„ç§æ•°æ®æºå’Œå·¥å…·
- âœ… **è°ƒåº¦èƒ½åŠ›**ï¼šå¼ºå¤§çš„è°ƒåº¦å’Œä¾èµ–ç®¡ç†
- âœ… **Web UI**ï¼šæä¾›ç›‘æ§å’Œç®¡ç†ç•Œé¢

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤§æ•°æ®ç®¡é“
- ETL å·¥ä½œæµ
- æ•°æ®å·¥ç¨‹ä»»åŠ¡
- å¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹

**ä»£ç ç¤ºä¾‹ï¼š**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def fetch_data():
    # è·å–æ•°æ®
    pass

def process_data():
    # å¤„ç†æ•°æ®
    pass

with DAG(
    'my_workflow',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
) as dag:
    fetch = PythonOperator(
        task_id='fetch_data',
        python_callable=fetch_data,
    )
    
    process = PythonOperator(
        task_id='process_data',
        python_callable=process_data,
    )
    
    fetch >> process
```

**æ¨èæŒ‡æ•°ï¼šâ­â­â­â­**

---

### 3. **Temporal** â­â­â­â­â­

**ç‰¹ç‚¹ï¼š**
- âœ… **åˆ†å¸ƒå¼å·¥ä½œæµ**ï¼šä¸“ä¸ºåˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡
- âœ… **å¯é æ€§**ï¼šå†…ç½®å®¹é”™å’Œé‡è¯•æœºåˆ¶
- âœ… **å¤šè¯­è¨€æ”¯æŒ**ï¼šPythonã€Goã€Javaã€TypeScript ç­‰
- âœ… **é•¿æœŸè¿è¡Œ**ï¼šæ”¯æŒé•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œæµ
- âœ… **çŠ¶æ€æŒä¹…åŒ–**ï¼šå·¥ä½œæµçŠ¶æ€è‡ªåŠ¨æŒä¹…åŒ–

**é€‚ç”¨åœºæ™¯ï¼š**
- å¾®æœåŠ¡ç¼–æ’
- åˆ†å¸ƒå¼ä»»åŠ¡åè°ƒ
- éœ€è¦é«˜å¯é æ€§çš„å·¥ä½œæµ
- é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡

**ä»£ç ç¤ºä¾‹ï¼š**
```python
from temporalio import workflow, activity
from datetime import timedelta

@activity
async def fetch_data():
    # è·å–æ•°æ®
    return data

@activity
async def process_data(data):
    # å¤„ç†æ•°æ®
    return processed_data

@workflow.defn
class MyWorkflow:
    @workflow.run
    async def run(self) -> str:
        data = await workflow.execute_activity(
            fetch_data,
            start_to_close_timeout=timedelta(seconds=10),
        )
        result = await workflow.execute_activity(
            process_data,
            data,
            start_to_close_timeout=timedelta(seconds=10),
        )
        return result
```

**æ¨èæŒ‡æ•°ï¼šâ­â­â­â­â­ï¼ˆé€‚åˆåˆ†å¸ƒå¼åœºæ™¯ï¼‰**

---

### 4. **Conductor** â­â­â­â­

**ç‰¹ç‚¹ï¼š**
- âœ… **Netflix å¼€æº**ï¼šç”± Netflix å¼€å‘å¹¶å¼€æº
- âœ… **JSON/YAML å®šä¹‰**ï¼šå·¥ä½œæµé€šè¿‡ JSON æˆ– YAML å®šä¹‰
- âœ… **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€
- âœ… **å¯è§†åŒ–**ï¼šæä¾›å·¥ä½œæµå¯è§†åŒ–ç•Œé¢
- âœ… **å¯æ‰©å±•**ï¼šæ”¯æŒè‡ªå®šä¹‰ä»»åŠ¡å’Œé›†æˆ

**é€‚ç”¨åœºæ™¯ï¼š**
- å¾®æœåŠ¡ç¼–æ’
- å¤æ‚ä¸šåŠ¡æµç¨‹
- éœ€è¦å¯è§†åŒ–çš„ä»£ç å®šä¹‰å·¥ä½œæµ

**æ¨èæŒ‡æ•°ï¼šâ­â­â­â­**

---

### 5. **Argo Workflows** â­â­â­â­

**ç‰¹ç‚¹ï¼š**
- âœ… **Kubernetes åŸç”Ÿ**ï¼šä¸“ä¸º Kubernetes è®¾è®¡
- âœ… **YAML å®šä¹‰**ï¼šä½¿ç”¨ YAML å®šä¹‰å·¥ä½œæµ
- âœ… **å®¹å™¨åŒ–**ï¼šæ¯ä¸ªæ­¥éª¤éƒ½æ˜¯å®¹å™¨
- âœ… **äº‘åŸç”Ÿ**ï¼šå®Œå…¨äº‘åŸç”Ÿæ¶æ„
- âœ… **CI/CD é›†æˆ**ï¼šé€‚åˆ CI/CD æµæ°´çº¿

**é€‚ç”¨åœºæ™¯ï¼š**
- Kubernetes ç¯å¢ƒ
- CI/CD æµæ°´çº¿
- å®¹å™¨åŒ–å·¥ä½œæµ
- äº‘åŸç”Ÿåº”ç”¨

**æ¨èæŒ‡æ•°ï¼šâ­â­â­â­ï¼ˆé€‚åˆ K8s ç¯å¢ƒï¼‰**

---

### 6. **LangGraph** â­â­â­â­ï¼ˆAI å·¥ä½œæµä¸“ç”¨ï¼‰

**ç‰¹ç‚¹ï¼š**
- âœ… **AI å·¥ä½œæµ**ï¼šä¸“ä¸º AI/LLM å·¥ä½œæµè®¾è®¡
- âœ… **å›¾ç»“æ„**ï¼šåŸºäºæœ‰å‘å›¾çš„å·¥ä½œæµå®šä¹‰
- âœ… **çŠ¶æ€ç®¡ç†**ï¼šå¼ºå¤§çš„çŠ¶æ€ä¼ é€’æœºåˆ¶
- âœ… **æ¡ä»¶åˆ†æ”¯**ï¼šæ”¯æŒå¤æ‚çš„æ¡ä»¶è·¯ç”±

**é€‚ç”¨åœºæ™¯ï¼š**
- AI åº”ç”¨å·¥ä½œæµ
- LLM ç¼–æ’
- å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
- ä½ å·²ç»åœ¨ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼

**æ¨èæŒ‡æ•°ï¼šâ­â­â­â­ï¼ˆAI åœºæ™¯ä¸“ç”¨ï¼‰**

---

## ğŸ“Š æ¡†æ¶å¯¹æ¯”è¡¨

| æ¡†æ¶ | è¯­è¨€ | å®šä¹‰æ–¹å¼ | é€‚ç”¨åœºæ™¯ | å­¦ä¹ æ›²çº¿ | æ¨èåº¦ |
|------|------|---------|---------|---------|--------|
| **Prefect** | Python | Python ä»£ç  | é€šç”¨å·¥ä½œæµã€ETL | ä½ | â­â­â­â­â­ |
| **Airflow** | Python | Python DAG | æ•°æ®ç®¡é“ã€ETL | ä¸­ | â­â­â­â­ |
| **Temporal** | å¤šè¯­è¨€ | ä»£ç  | åˆ†å¸ƒå¼å·¥ä½œæµ | ä¸­é«˜ | â­â­â­â­â­ |
| **Conductor** | å¤šè¯­è¨€ | JSON/YAML | å¾®æœåŠ¡ç¼–æ’ | ä¸­ | â­â­â­â­ |
| **Argo Workflows** | YAML | YAML | K8s å·¥ä½œæµ | ä¸­é«˜ | â­â­â­â­ |
| **LangGraph** | Python | Python ä»£ç  | AI å·¥ä½œæµ | ä¸­ | â­â­â­â­ |

---

## ğŸ¯ é€‰æ‹©å»ºè®®

### å¦‚æœä½ éœ€è¦**é€šç”¨å·¥ä½œæµè‡ªåŠ¨åŒ–**ï¼ˆç±»ä¼¼ Activepiecesï¼‰

**æ¨èï¼šPrefect**

**åŸå› ï¼š**
- âœ… æœ€æ¥è¿‘ Activepieces çš„åŠŸèƒ½å®šä½
- âœ… ä»£ç ä¼˜å…ˆï¼ŒAPI ä¼˜é›…
- âœ… æ”¯æŒå„ç§é›†æˆå’Œè§¦å‘å™¨
- âœ… æ˜“äºå­¦ä¹ å’Œä½¿ç”¨
- âœ… æ´»è·ƒçš„ç¤¾åŒºå’Œæ–‡æ¡£

**ç¤ºä¾‹åœºæ™¯ï¼š**
- API é›†æˆå’Œè‡ªåŠ¨åŒ–
- æ•°æ®åŒæ­¥ä»»åŠ¡
- å®šæ—¶ä»»åŠ¡è°ƒåº¦
- å·¥ä½œæµç¼–æ’

---

### å¦‚æœä½ éœ€è¦**æ•°æ®ç®¡é“å’Œ ETL**

**æ¨èï¼šAirflow æˆ– Prefect**

- **Airflow**ï¼šå¦‚æœä½ éœ€è¦æˆç†Ÿç¨³å®šçš„è§£å†³æ–¹æ¡ˆï¼Œå›¢é˜Ÿç†Ÿæ‚‰ Airflow
- **Prefect**ï¼šå¦‚æœä½ æƒ³è¦æ›´ç°ä»£çš„ API å’Œæ›´å¥½çš„å¼€å‘ä½“éªŒ

---

### å¦‚æœä½ éœ€è¦**åˆ†å¸ƒå¼å·¥ä½œæµ**

**æ¨èï¼šTemporal**

**åŸå› ï¼š**
- âœ… ä¸“ä¸ºåˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡
- âœ… é«˜å¯é æ€§
- âœ… æ”¯æŒé•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œæµ
- âœ… å¤šè¯­è¨€æ”¯æŒ

---

### å¦‚æœä½ åœ¨ **Kubernetes ç¯å¢ƒ**

**æ¨èï¼šArgo Workflows**

**åŸå› ï¼š**
- âœ… Kubernetes åŸç”Ÿ
- âœ… å®¹å™¨åŒ–å·¥ä½œæµ
- âœ… é€‚åˆäº‘åŸç”Ÿæ¶æ„

---

### å¦‚æœä½ éœ€è¦ **AI å·¥ä½œæµ**

**æ¨èï¼šLangGraph**

**åŸå› ï¼š**
- âœ… ä¸“ä¸º AI/LLM è®¾è®¡
- âœ… ä½ å·²ç»åœ¨ä½¿ç”¨
- âœ… å¼ºå¤§çš„çŠ¶æ€ç®¡ç†å’Œæ¡ä»¶åˆ†æ”¯

---

## ğŸ’¡ å®é™…åº”ç”¨ç¤ºä¾‹

### åœºæ™¯ 1ï¼šAPI é›†æˆå·¥ä½œæµï¼ˆç±»ä¼¼ Activepiecesï¼‰

**ä½¿ç”¨ Prefectï¼š**

```python
from prefect import flow, task
from prefect.blocks.system import Secret
import requests

@task
def fetch_from_api_a():
    # ä» API A è·å–æ•°æ®
    api_key = Secret.load("api-a-key")
    response = requests.get("https://api-a.com/data", headers={"Authorization": api_key.get()})
    return response.json()

@task
def transform_data(data):
    # è½¬æ¢æ•°æ®
    return transformed_data

@task
def send_to_api_b(data):
    # å‘é€åˆ° API B
    api_key = Secret.load("api-b-key")
    requests.post("https://api-b.com/data", json=data, headers={"Authorization": api_key.get()})

@flow
def api_integration_workflow():
    data = fetch_from_api_a()
    transformed = transform_data(data)
    send_to_api_b(transformed)

# å®šæ—¶æ‰§è¡Œ
if __name__ == "__main__":
    api_integration_workflow()
```

---

### åœºæ™¯ 2ï¼šæ•°æ®å¤„ç†ç®¡é“

**ä½¿ç”¨ Airflowï¼š**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'data_pipeline',
    default_args=default_args,
    description='æ•°æ®å¤„ç†ç®¡é“',
    schedule_interval='@daily',
) as dag:
    
    extract = PythonOperator(
        task_id='extract_data',
        python_callable=extract_function,
    )
    
    transform = PythonOperator(
        task_id='transform_data',
        python_callable=transform_function,
    )
    
    load = PostgresOperator(
        task_id='load_to_db',
        postgres_conn_id='postgres_default',
        sql='INSERT INTO table VALUES ...',
    )
    
    extract >> transform >> load
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹æ¨è

### å¯¹äºå¤§å¤šæ•°åœºæ™¯ï¼Œæˆ‘æ¨èä» **Prefect** å¼€å§‹ï¼š

1. **å®‰è£…ï¼š**
```bash
pip install prefect
```

2. **ç®€å•ç¤ºä¾‹ï¼š**
```python
from prefect import flow, task

@task
def say_hello():
    return "Hello, World!"

@flow
def my_flow():
    result = say_hello()
    print(result)

if __name__ == "__main__":
    my_flow()
```

3. **è¿è¡Œï¼š**
```bash
python my_flow.py
```

---

## ğŸ“š å­¦ä¹ èµ„æº

- **Prefect**: [å®˜æ–¹æ–‡æ¡£](https://docs.prefect.io/) | [GitHub](https://github.com/PrefectHQ/prefect)
- **Airflow**: [å®˜æ–¹æ–‡æ¡£](https://airflow.apache.org/docs/) | [GitHub](https://github.com/apache/airflow)
- **Temporal**: [å®˜æ–¹æ–‡æ¡£](https://docs.temporal.io/) | [GitHub](https://github.com/temporalio/temporal)
- **Conductor**: [GitHub](https://github.com/Netflix/conductor)
- **Argo Workflows**: [å®˜æ–¹æ–‡æ¡£](https://argoproj.github.io/workflows/) | [GitHub](https://github.com/argoproj/argo-workflows)

---

## ğŸ¯ æ€»ç»“

**å¦‚æœä½ æƒ³è¦ç±»ä¼¼ Activepieces ä½†ä»£ç ä¼˜å…ˆçš„æ¡†æ¶ï¼š**

1. **é¦–é€‰ï¼šPrefect** - æœ€æ¥è¿‘ Activepieces çš„å®šä½ï¼Œä»£ç ä¼˜å…ˆï¼Œæ˜“äºä½¿ç”¨
2. **æ¬¡é€‰ï¼šAirflow** - å¦‚æœå›¢é˜Ÿç†Ÿæ‚‰ï¼Œéœ€è¦æˆç†Ÿç¨³å®šçš„æ–¹æ¡ˆ
3. **åˆ†å¸ƒå¼åœºæ™¯ï¼šTemporal** - å¦‚æœéœ€è¦é«˜å¯é æ€§çš„åˆ†å¸ƒå¼å·¥ä½œæµ
4. **K8s ç¯å¢ƒï¼šArgo Workflows** - å¦‚æœåœ¨ Kubernetes ç¯å¢ƒ
5. **AI åœºæ™¯ï¼šLangGraph** - ä½ å·²ç»åœ¨ä½¿ç”¨ï¼Œé€‚åˆ AI å·¥ä½œæµ

**å»ºè®®ï¼šä» Prefect å¼€å§‹å°è¯•ï¼Œå®ƒæœ€ç¬¦åˆä½ çš„éœ€æ±‚ï¼**

